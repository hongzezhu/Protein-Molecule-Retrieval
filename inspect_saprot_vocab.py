from transformers import AutoTokenizer
import os

# 1. åŠ è½½ Tokenizer

model_path = "westlake-repl/SaProt_650M_AF2" 
print(f">>> Loading tokenizer from {model_path}...")

try:
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
except Exception as e:
    print(f"Error loading tokenizer: {e}")
    exit()

# 2. å®šä¹‰æœç´¢ç©ºé—´
amino_acids = "ACDEFGHIKLMNPQRSTVWY" # 20ç§æ ‡å‡†æ°¨åŸºé…¸
foldseek_chars = "abcdefghijklmnopqrstuvwxyz" # å¯èƒ½çš„ç»“æ„å­—ç¬¦

print("\n>>> æ­£åœ¨æ‰«æè¯è¡¨ï¼Œå¯»æ‰¾åˆæ³•çš„ [AA+Struct] ç»„åˆ...")
print("="*60)

# 3. å¯»æ‰¾ä¸‡èƒ½è¡¥ä¸
universal_struct = None

# æˆ‘ä»¬å…ˆæ‰“å°å‡ ä¸ªæ¥çœ‹çœ‹ Token é•¿ä»€ä¹ˆæ ·
print("Previewing some valid tokens for 'M' (Methionine):")
example_found = False
for char in foldseek_chars:
    # æµ‹è¯•å‡ ç§å¸¸è§æ ¼å¼
    candidates = [f"M{char}", f"M#{char}", f"{char}M"]
    for c in candidates:
        ids = tokenizer.encode(c, add_special_tokens=False)
        if len(ids) == 1 and 3 not in ids: # id 3 usually UNK
            print(f"  âœ… Valid Token Found: '{c}' -> ID {ids[0]}")
            example_found = True
if not example_found:
    print("  âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½• 'M' çš„ç»„åˆ Tokenï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ vocab.txt æ–‡ä»¶å†…å®¹")

print("-" * 60)

# 4. å¯»æ‰¾ä¸€ä¸ªå¯¹æ‰€æœ‰ 20 ç§æ°¨åŸºé…¸éƒ½æœ‰æ•ˆçš„ç»“æ„å­—ç¬¦
for s in foldseek_chars:
    is_universal = True
    # å‡è®¾æ ¼å¼æ˜¯ "Mc" (å¤§å†™AA + å°å†™Struct)ï¼Œè¿™æ˜¯æœ€å¸¸è§çš„ Foldseek-PLM æ ¼å¼
    # æˆ‘ä»¬ä¹Ÿä¼šæµ‹è¯• "M#c"
    
    # è‡ªåŠ¨æ¢æµ‹æ ¼å¼
    format_template = None
    
    # å…ˆæ¢æµ‹æ ¼å¼
    test_aa = "M"
    if tokenizer.convert_tokens_to_ids(f"{test_aa}{s}") != tokenizer.unk_token_id:
        format_template = "{aa}{s}" # æ ¼å¼å¦‚ Mc
    elif tokenizer.convert_tokens_to_ids(f"{test_aa}#{s}") != tokenizer.unk_token_id:
        format_template = "{aa}#{s}" # æ ¼å¼å¦‚ M#c
    
    if not format_template:
        continue # è¿™ä¸ªç»“æ„å­—ç¬¦ s è¿ M éƒ½åŒ¹é…ä¸ä¸Šï¼Œè·³è¿‡

    # éªŒè¯æ˜¯å¦è¦†ç›–æ‰€æœ‰æ°¨åŸºé…¸
    for aa in amino_acids:
        token = format_template.format(aa=aa, s=s)
        idx = tokenizer.convert_tokens_to_ids(token)
        if idx == tokenizer.unk_token_id:
            is_universal = False
            break
    
    if is_universal:
        print(f"\nğŸ‰ æ‰¾åˆ°äº†ä¸‡èƒ½ç»“æ„åç¼€: '{s}'")
        print(f"ğŸ‰ ç¡®å®šçš„ Token æ ¼å¼: '{format_template.format(aa='M', s=s)}'")
        print(f"   (è¿™æ„å‘³ç€ä½ å¯ä»¥ç”¨ '{format_template.format(aa='aa', s=s)}' æ¥è¡¥å…¨æ‰€æœ‰åºåˆ—)")
        universal_struct = (s, format_template)
        break

if not universal_struct:
    print("\nâŒ æœªæ‰¾åˆ°å•ä¸€çš„ä¸‡èƒ½ç»“æ„å­—ç¬¦ã€‚æˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªæ˜ å°„è¡¨ã€‚")
    print("æ­£åœ¨ç”Ÿæˆå®‰å…¨æ˜ å°„è¡¨...")
    safe_map = {}
    for aa in amino_acids:
        for s in foldseek_chars:
            # ä¼˜å…ˆæµ‹è¯•æ— äº•å·æ ¼å¼ "Mc"
            token = f"{aa}{s}"
            if tokenizer.convert_tokens_to_ids(token) != tokenizer.unk_token_id:
                safe_map[aa] = token
                break
            # æµ‹è¯•å¸¦äº•å· "M#c"
            token_hash = f"{aa}#{s}"
            if tokenizer.convert_tokens_to_ids(token_hash) != tokenizer.unk_token_id:
                safe_map[aa] = token_hash
                break
        
        if aa not in safe_map:
            print(f"  âš ï¸ è­¦å‘Š: æ°¨åŸºé…¸ {aa} æ‰¾ä¸åˆ°ä»»ä½•åˆæ³•ç»“æ„ Tokenï¼")
    
    print("\nâœ… ç”Ÿæˆäº†å®‰å…¨æ˜ å°„ map (éƒ¨åˆ†å±•ç¤º):")
    print(list(safe_map.items())[:5])
